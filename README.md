# Efficient Battery Energy Storage Control Using Decision Transformers and Knowledge Distillation
The integration of renewable energy sources (RES), particularly photovoltaic (PV) systems, introduces considerable variability and temporal imbalance between energy generation and consumption. Although traditional demand-side management (DSM) strategies have gained increasing attention in recent years, consumer behaviour patterns and the lack of automation and control technologies limit their effectiveness. In this context, residential battery energy storage systems (BESS) play a crucial role in improving energy self-consumption and enhancing grid stability. Incorporated with dynamic electricity pricing, advanced scheduling algorithms can predict consumption patterns and optimise battery operation, enabling more efficient energy management and better alignment with variable RES.

However, traditional control methods, such as rule-based algorithms, stochastic models or model predictive control, often require detailed system modelling and may lack the flexibility to adapt to complex or unseen conditions. In response, recent research has increasingly focused on machine learning-based approaches. Among these, reinforcement learning has emerged as a promising method due to its capability to learn optimal control policies directly from interaction data without requiring explicit system models. Among recent RL developments, Decision Transformers (DTs) have emerged as a novel approach to framing decision-making as a conditional sequence modelling problem. In this process, the transformer architecture from natural language processing is adapted to predict actions from past trajectories and target returns. This approach offers a more stable and goal-conditioned framework that can enhance efficiency compared to conventional RL methods. Despite the advantages of DTs, their large model size and high computational demands hinder deployment in embedded and resource-constrained environments, such as residential energy controllers. Therefore, knowledge distillation (KD) has been proposed to mitigate these limitations by transferring knowledge from a large to a smaller model, enabling deployment with minimal performance degradation.
